{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy.random as random\n",
    "import re\n",
    "\n",
    "# note: we can build our own char base from reading the file\n",
    "all_chars = string.printable\n",
    "n_chars = len(all_chars) # total number of characters\n",
    "\n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text_len = len(text)\n",
    "print('text_len =', text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re in all my life.\n",
      "\n",
      "VINCENTIO:\n",
      "What, you notorious villain, didst thou never see\n",
      "thy master's father, Vincentio?\n",
      "\n",
      "BIONDELLO:\n",
      "What, my old worshipful old master? yes, marry, sir:\n",
      "see where he looks out\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "\n",
    "def random_seq():\n",
    "    start = random.randint(0, text_len - seq_len + 1) # numpy random gives int [low, high) hence the +1\n",
    "    end = start + seq_len\n",
    "    return text[start:end]\n",
    "\n",
    "print(random_seq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharSeqRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size # number of chars for this case\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim # we could keep this same as hidden dim to reduce one variable\n",
    "        \n",
    "        self.encode = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=1, batch_first=True) # we can try dropout\n",
    "        self.decode = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        inp = self.encode(inp) #input must be N x T\n",
    "        output, hidden = self.rnn(inp, hidden)\n",
    "        output = self.decode(output)\n",
    "        #output = F.log_softmax(output, dim=2) # we can  do this at output\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(1, batch_size, self.hidden_dim)).cuda(),\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_dim)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 10  11  12  39  40  41\n",
      "[torch.cuda.LongTensor of size 1x6 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def char_index(chars):\n",
    "    return Variable(torch.LongTensor([all_chars.index(c) for c in chars]).view(1,-1)).cuda()\n",
    "print(char_index(\"abcDEF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch(batch_size):\n",
    "    chars_in = []\n",
    "    chars_out = []\n",
    "    for i in range(batch_size):\n",
    "        char_seq = random_seq()\n",
    "        chars_in.append(char_index(char_seq[:-1]))\n",
    "        chars_out.append(char_index(char_seq[1:]))\n",
    "    chars_in = torch.cat(chars_in, dim=0).cuda()\n",
    "    chars_out = torch.cat(chars_out, dim=0).cuda()\n",
    "    return chars_in, chars_out\n",
    "\n",
    "c_in, c_out = training_batch(1)\n",
    "#print(c_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(init_str='A', length=200, temp=0.4):\n",
    "    hidden = model.init_hidden(1)\n",
    "    pred = init_str\n",
    "    if len(init_str) > 1:\n",
    "        input = char_index(init_str[:-1])\n",
    "        _, hidden = model(input, hidden)\n",
    "    \n",
    "    input = char_index(init_str[-1])\n",
    "    \n",
    "    for i in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        \n",
    "        output_dist = F.softmax(output.view(-1)/temp, dim=0).data\n",
    "        idx = torch.multinomial(output_dist, 1)[0]\n",
    "        pred_char = all_chars[idx]\n",
    "        pred += pred_char\n",
    "        input = char_index(pred_char)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    c_in, c_out = training_batch(batch_size)\n",
    "    \n",
    "    output, hidden = model(c_in, hidden)\n",
    "    loss = criterion(output.view(-1, n_chars), c_out.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 6s (100 5%) 2.7967]\n",
      "\n",
      "Aky?UCvNF ine the that se\n",
      "\n",
      " he yl wf facr h leer ter oos net corous the we hone win me are wo ihe wd ues a h\n",
      "\n",
      "e tat ind linsm in he is mon yed din sig \n",
      "\n",
      "[0m 13s (200 10%) 2.4326]\n",
      "\n",
      "ELb'll the dory this therd he ores me save that bave hare he mand ou the the wand, he ond men math the beare so the wat the as you ine me the mer an t \n",
      "\n",
      "[0m 19s (300 15%) 2.2405]\n",
      "\n",
      "FENE:\n",
      "The sart,\n",
      "And wit the the ther mis and seres the werens and os the frit your the the ther rothe my me to in the fere srorst ant the werem the me \n",
      "\n",
      "[0m 25s (400 20%) 2.1572]\n",
      "\n",
      "AS, are som all souly hat the sow sour I sin fore hat and couls the pare my and it uld fard thear the as at the for to is the hinging ond th the the s \n",
      "\n",
      "[0m 31s (500 25%) 2.0963]\n",
      "\n",
      "Goud for the the the the me to me and will be preach the stere lith the and surienind shall more sied and to the more seell the mowe hours not wath th \n",
      "\n",
      "[0m 37s (600 30%) 2.0122]\n",
      "\n",
      "Sis the that lords the the mand, yer in the wing the will the all thou the llouss,\n",
      "And streard.\n",
      "\n",
      "BADINGANTIO:\n",
      "The the have so the he the hore the ere  \n",
      "\n",
      "[0m 43s (700 35%) 1.9910]\n",
      "\n",
      "And of the lords well the conthould be on the a to thou the since the of and in the mare be wo some and me the the my hour shour some prost had the me \n",
      "\n",
      "[0m 49s (800 40%) 1.8981]\n",
      "\n",
      "For the staint seave the well be to the shill and them my mand all not thou with the to deather have for the come the our his heart mase have and you  \n",
      "\n",
      "[0m 55s (900 45%) 1.9100]\n",
      "\n",
      "The mant, the be with thear that the the the hat doon, be thee, wath soan the not the all singer of the suld and is the lave,\n",
      "And with the stake you p \n",
      "\n",
      "[1m 1s (1000 50%) 1.8622]\n",
      "\n",
      "Thou sirge, stome his sing the with have to the dear me true the poods to see, you the stay the manter is to come and was is me that the maring the th \n",
      "\n",
      "[1m 7s (1100 55%) 1.8182]\n",
      "\n",
      "IS what with thour the have of counther and all time the aspand of hath and the so mest prows of her the sorse.\n",
      "\n",
      "RICHARD III:\n",
      "Which say, I is shill ha \n",
      "\n",
      "[1m 13s (1200 60%) 1.7901]\n",
      "\n",
      "'The set the to he a son the all me my shall and the say have a for the lords the sward to seend and is it the master the did course pood to her the s \n",
      "\n",
      "[1m 19s (1300 65%) 1.7737]\n",
      "\n",
      "Do the word is that and hee more strait, and lise of on the dangert,\n",
      "And there mate death the have a congent?\n",
      "\n",
      "Second I the sinders:\n",
      "The good love him \n",
      "\n",
      "[1m 25s (1400 70%) 1.7509]\n",
      "\n",
      "God a diend the sead and some and the wand and shall well some as thou some so the prother,\n",
      "And be will the gore and have the have the prother,\n",
      "To hav \n",
      "\n",
      "[1m 32s (1500 75%) 1.6812]\n",
      "\n",
      "\n",
      "LEONTES:\n",
      "My down the dear in thy make of the deaven of thy shount your make her leathes and that a beanger that say for beart her the lord,\n",
      "And the s \n",
      "\n",
      "[1m 38s (1600 80%) 1.7448]\n",
      "\n",
      "Well have and the to be can to were you grave in the lords the commands the reter the some made this my have can the bear it entering the lown, and so \n",
      "\n",
      "[1m 44s (1700 85%) 1.7011]\n",
      "\n",
      "Proved in the heart the man such true it and a privient the bedouse of stand the stale of the see the report he me of think the done shall be pore, an \n",
      "\n",
      "[1m 50s (1800 90%) 1.6671]\n",
      "\n",
      "If the sire the grace to gare and him,\n",
      "To the goren and they here deet in the string, well with let as with the consure with mine of I have words of h \n",
      "\n",
      "[1m 56s (1900 95%) 1.6979]\n",
      "\n",
      "Firs in the sirs of your from the plainess lord.\n",
      "\n",
      "PETRUCHIO:\n",
      "We may with carrien the hards;\n",
      "And sead of the stand the respiech stand the counting.\n",
      "\n",
      "WA \n",
      "\n",
      "[2m 2s (2000 100%) 1.6659]\n",
      "\n",
      "And the comes distain beet my friends our well with the course his man my threes and her good the their stall the father in the good better rester.\n",
      "\n",
      "S \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "print_fq = 100\n",
    "plot_fq = 10\n",
    "\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "batch_size = 64\n",
    "model = CharSeqRNN(n_chars, embed_dim, hidden_dim).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss1 = train(batch_size)\n",
    "    loss_avg += loss1\n",
    "    if epoch % print_fq == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss1))\n",
    "        print(run('\\n', 150, 0.5), '\\n')\n",
    "\n",
    "    if epoch % plot_fq == 0:\n",
    "        losses.append(loss_avg / plot_fq)\n",
    "        loss_avg = 0\n",
    "    \n",
    "#print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training\n",
      "\n",
      "The seen the such the grace the string the sorrow the stranger for the sorrow the man the stranger the stand the lord,\n",
      "The stranger for the so make this so man the shall the such the bear the provest the so man the such the such of the so make the pringed the courter the stranger to me and the sone to the such the such the such a son the bear a promise and the sone to the son and the stranger the stand the stranger me to me the proves to me the prince of the will the sorrow the so man the couse \n"
     ]
    }
   ],
   "source": [
    "print(\"After training\")\n",
    "print(run('\\n', 500, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model parameters\n",
    "state = {\n",
    "            'args': (n_chars, embed_dim, hidden_dim), \n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "\n",
    "torch.save(state, 'model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
