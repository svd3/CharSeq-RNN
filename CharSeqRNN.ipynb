{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy.random as random\n",
    "import re\n",
    "\n",
    "# note: we can build our own char base from reading the file\n",
    "all_chars = string.printable\n",
    "n_chars = len(all_chars) # total number of characters\n",
    "\n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text_len = len(text)\n",
    "print('text_len =', text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws of your husband.\n",
      "\n",
      "VIRGILIA:\n",
      "O, good madam, there can be none yet.\n",
      "\n",
      "VALERIA:\n",
      "Verily, I do not jest with you; there came news from\n",
      "him last night.\n",
      "\n",
      "VIRGILIA:\n",
      "Indeed, madam?\n",
      "\n",
      "VALERIA:\n",
      "In earnest, it's\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "\n",
    "def random_seq():\n",
    "    start = random.randint(0, text_len - seq_len + 1) # numpy random gives int [low, high) hence the +1\n",
    "    end = start + seq_len\n",
    "    return text[start:end]\n",
    "\n",
    "print(random_seq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharSeqRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size # number of chars for this case\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim # we could keep this same as hidden dim to reduce one variable\n",
    "        \n",
    "        self.encode = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=1, batch_first=True) # we can try dropout\n",
    "        self.decode = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        inp = self.encode(inp) #input must be N x T\n",
    "        output, hidden = self.rnn(inp, hidden)\n",
    "        output = self.decode(output)\n",
    "        #output = F.log_softmax(output, dim=2) # we can  do this at output\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(1, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 10  11  12  39  40  41\n",
      "[torch.LongTensor of size (1,6)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def char_index(chars):\n",
    "    return Variable(torch.LongTensor([all_chars.index(c) for c in chars]).view(1,-1))\n",
    "\n",
    "print(char_index(\"abcDEF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_batch(batch_size):\n",
    "    chars_in = []\n",
    "    chars_out = []\n",
    "    for i in range(batch_size):\n",
    "        char_seq = random_seq()\n",
    "        chars_in.append(char_index(char_seq[:-1]))\n",
    "        chars_out.append(char_index(char_seq[1:]))\n",
    "    chars_in = torch.cat(chars_in, dim=0)\n",
    "    chars_out = torch.cat(chars_out, dim=0)\n",
    "    return chars_in, chars_out\n",
    "\n",
    "c_in, c_out = training_batch(1)\n",
    "#print(c_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(init_str='A', length=200, temp=0.4):\n",
    "    hidden = model.init_hidden(1)\n",
    "    pred = init_str\n",
    "    if len(init_str) > 1:\n",
    "        input = char_index(init_str[:-1])\n",
    "        _, hidden = model(input, hidden)\n",
    "    \n",
    "    input = char_index(init_str[-1])\n",
    "    \n",
    "    for i in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        \n",
    "        output_dist = F.softmax(output.view(-1)/temp, dim=0).data\n",
    "        idx = torch.multinomial(output_dist, 1)[0]\n",
    "        pred_char = all_chars[idx]\n",
    "        pred += pred_char\n",
    "        input = char_index(pred_char)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    c_in, c_out = training_batch(batch_size)\n",
    "    \n",
    "    output, hidden = model(c_in, hidden)\n",
    "    loss = criterion(output.view(-1, n_chars), c_out.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4m 16s (100 5%) 2.4376]\n",
      "\n",
      "Whine dee hos thee to wine pan inbet the thou thas that the the wis ang sothen ind dout the bors all that de path he lot weran the rot hie son nous co \n",
      "\n",
      "[8m 41s (200 10%) 2.1566]\n",
      "\n",
      "REHKE INGTE:\n",
      "And this the cance mand the with rise mot har this herat is the sher the the my thou the of and lo prate shath beat on the will so madis  \n",
      "\n",
      "[13m 22s (300 15%) 2.0317]\n",
      "\n",
      "I I freresence hat the thim of hat the seed of well her sofe it the lord and with the the the here my morterd,\n",
      "And shat lith the gordon and frow here  \n",
      "\n",
      "[17m 57s (400 20%) 1.9411]\n",
      "\n",
      "Aw my be that not.\n",
      "\n",
      "MENCETIO:\n",
      "For the the shat my the stir.\n",
      "\n",
      "CLAUSTES:\n",
      "Sern the maded you poors, lone for fall the so searter Cander and dongue the hi \n",
      "\n",
      "[22m 0s (500 25%) 1.8832]\n",
      "\n",
      "Hor to say be not your his to my to hersenty,\n",
      "\n",
      "First our stand that our so the some my be him and and cone is the reath the parding but the shall.\n",
      "\n",
      "GL \n",
      "\n",
      "[26m 2s (600 30%) 1.8275]\n",
      "\n",
      "Then a moor should to me the she will in this purvest,\n",
      "Whild he sinder the so his from here the will ame gentles the will the words, what the here\n",
      "Aad \n",
      "\n",
      "[30m 12s (700 35%) 1.7768]\n",
      "\n",
      "The some a main the shall in the spick, so now hand me him her,\n",
      "And beast the are were you see her stand,\n",
      "And the will the so make of the last in the  \n",
      "\n",
      "[34m 17s (800 40%) 1.7279]\n",
      "\n",
      "The his compation whits are the countrent.\n",
      "\n",
      "ENSIO:\n",
      "I more thou all the print the grance\n",
      "The reasence I have as then shall the such and a dears;\n",
      "What w \n",
      "\n",
      "[38m 20s (900 45%) 1.7204]\n",
      "\n",
      "The may is the shon thou to may that parson\n",
      "The more to when the see of for with the word,\n",
      "The beat that heart the counting the heart\n",
      "To happer the se \n",
      "\n",
      "[42m 36s (1000 50%) 1.6506]\n",
      "\n",
      "Whou have vose to the conder the wordly.\n",
      "\n",
      "JULIET:\n",
      "And to purse the pricks of his come the father,\n",
      "Where should have a call the have contime Came\n",
      "That  \n",
      "\n",
      "[47m 4s (1100 55%) 1.6566]\n",
      "\n",
      "The fills of my good shall not for says true the place\n",
      "That with the come the stay, and be great, and he the will offort and word,\n",
      "And sir, and have n \n",
      "\n",
      "[51m 28s (1200 60%) 1.6409]\n",
      "\n",
      "Hear sould be the father the hours,\n",
      "The hissean a consure sir, and the ears to have so his dost is not\n",
      "We have is not for the worded to your scound,\n",
      "A \n",
      "\n",
      "[56m 26s (1300 65%) 1.5825]\n",
      "\n",
      "Here many I leaties, but the rest the death\n",
      "That the hours of the seem to sick and say,\n",
      "And come the place to him hand the bast by the stands,\n",
      "And whe \n",
      "\n",
      "[61m 31s (1400 70%) 1.5979]\n",
      "\n",
      "CAMILO:\n",
      "Well the would stay me to me assing of the come.\n",
      "\n",
      "PETRUCHIO:\n",
      "Marry, my lord of the sould be such out and I have a back of my partion\n",
      "From the  \n",
      "\n",
      "[66m 22s (1500 75%) 1.5678]\n",
      "\n",
      "By dost the house thou hast is had with the cheing here\n",
      "Wither us the day the compless.\n",
      "\n",
      "POLIXENES:\n",
      "I do thou do in the book and dones,\n",
      "Which so this  \n",
      "\n",
      "[70m 42s (1600 80%) 1.5783]\n",
      "\n",
      "O, good on the world me from the peace.\n",
      "\n",
      "COMINIUS:\n",
      "I have resold at the bear the strike.\n",
      "\n",
      "LUCENTIO:\n",
      "That here hath sented the grace mine in the earth, \n",
      "\n",
      "[75m 36s (1700 85%) 1.5235]\n",
      "\n",
      "What have not this friend and she stands me in this consures\n",
      "Is my gentleman and give the death me.\n",
      "\n",
      "KING RICHARD III:\n",
      "And we should still of my son t \n",
      "\n",
      "[80m 44s (1800 90%) 1.5542]\n",
      "\n",
      "Of the heard of the morter to my comments them,\n",
      "And see the say to much out to the death to be such a worth\n",
      "The such all the that think a dones,\n",
      "Which \n",
      "\n",
      "[85m 22s (1900 95%) 1.5331]\n",
      "\n",
      "Not me to my grace that a speak to me\n",
      "That prove the will of my tongue thee should was and the world.\n",
      "\n",
      "BIANCA:\n",
      "Now, the man to a man his some would to \n",
      "\n",
      "[90m 36s (2000 100%) 1.5463]\n",
      "\n",
      "What and streake on the good that all the grace and she sleep,\n",
      "And the shild to make 'way he show a country, the maid\n",
      "And san my consent to this how y \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "print_fq = 100\n",
    "plot_fq = 10\n",
    "\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "batch_size = 64\n",
    "model = CharSeqRNN(n_chars, embed_dim, hidden_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss1 = train(batch_size)\n",
    "    loss_avg += loss1\n",
    "    if epoch % print_fq == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss1))\n",
    "        print(run('\\n', 150, 0.5), '\\n')\n",
    "\n",
    "    if epoch % plot_fq == 0:\n",
    "        losses.append(loss_avg / plot_fq)\n",
    "        loss_avg = 0\n",
    "    \n",
    "#print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training\n",
      "\n",
      "The stands and the son the bears to the cousin,\n",
      "That a stand with the country to the partion of the country.\n",
      "\n",
      "PETRUCHIO:\n",
      "I will not stand and the straition to the partion to the such all the words,\n",
      "And the sen the such the stand of the words,\n",
      "And the complest to the stand and such made and the father\n",
      "That the father with the world with the prince,\n",
      "The sen the world and the common made and the stand and the partion.\n",
      "\n",
      "BRUTUS:\n",
      "Why, the send the consul of the house to the seem the prince\n",
      "That with t\n"
     ]
    }
   ],
   "source": [
    "print(\"After training\")\n",
    "print(run('\\n', 500, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model parameters\n",
    "state = {\n",
    "            'args': (n_chars, embed_dim, hidden_dim), \n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "\n",
    "torch.save(state, 'pretrained/model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
